# -*- coding: utf-8 -*-
"""LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lN9FJu6IlSpbtDCeVmVlfB7PLis3Almt
"""

#to predict the next word using deep learning models

# -*- coding: utf-8 -*-
"""
LSTM model on Shakespeare's Hamlet using NLTK Gutenberg corpus.
"""

import nltk
import pandas as pd
import numpy as np
import tensorflow as tf
from nltk.corpus import gutenberg
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from sklearn.model_selection import train_test_split

# Download corpus if not already available
nltk.download("gutenberg")

# Load Hamlet text from Gutenberg
text = gutenberg.raw("shakespeare-hamlet.txt")

# Print available file ids for verification
print("Available texts:", gutenberg.fileids())

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
total_words = len(tokenizer.word_index) + 1
print("Total unique words:", total_words)

# Generate input sequences for training (n-grams)
sequences = []
for line in text.split('\n'):
    token_list = tokenizer.texts_to_sequences([line])[0]
    for i in range(1, len(token_list)):
        n_gram_sequence = token_list[:i + 1]
        sequences.append(n_gram_sequence)

print("Total sequences generated:", len(sequences))

# Pad sequences to uniform length (padding)
max_seq_len = max([len(seq) for seq in sequences])
sequences = pad_sequences(sequences, maxlen=max_seq_len, padding="pre")
max_seq_len

input_sequences_pre = pad_sequences(sequences,maxlen=max_seq_len, padding="pre")

input_sequences_pre

#split the features(x) and the labels (y)
#each sequence should be split so that the last word is the label (target output) and the rest are the input
x = input_sequences_pre[:,:-1]
y = input_sequences_pre[:,-1]
x
y

#from the looks of it, y is just word indexes ,we need to 1 hot encode it to match vocab size

from tensorflow.keras.utils import to_categorical
y = to_categorical(y, num_classes=total_words)
y

x_train,y_train,x_test,y_test= train_test_split(x,y,test_size=0.2)

#to prevent overfitting, we need to control the number of epochs
from tensorflow.keras.callbacks import EarlyStopping
early_stop = EarlyStopping(monitor='val_loss',patience=3, restore_best_weights=True)
#patience = 3 means no improvement after 3 epochs, best weights true means keep best weights and not the last ones